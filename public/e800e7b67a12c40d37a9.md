---
title: YOLO-segをファインチューニングして「人検知＋向き推定」を1モデルに統合する
tags:
  - Python
  - 画像認識
  - YOLO
  - segmentation
private: false
updated_at: '2026-02-08T01:40:57+09:00'
id: e800e7b67a12c40d37a9
organization_url_name: null
slide: false
ignorePublish: false
---

今回はYOLO26s-segをファインチューニングして、歩行者セグメンテーションと向き推定を同時に行うモデルを獲得してみました。
ソースコードは[こちら](https://github.com/DaichiKiyozaki/seg-person-dir)です。

## 背景

### 従来の構成

研究要件として、歩行者のセグメンテーションと、向き推定（カメラと同じ方向を向いているか否か）が必要でした。以前は以下の2モデル構成を想定していました。

* **YOLO-seg**

  * 人のインスタンスセグメンテーション（人検知）
* **MEBOW**

  * 各人物について「体の向き（前後）」を推定

つまり、

> 検出モデル + 向き推定モデル = 2モデル構成

です。

しかし、この構成には実運用上いくつか問題がありました。

* モデルが **2つ** ある（運用・依存関係が増える）
* MEBOWは **1人ずつ推論**するため、人が増えると計算コストが増える

今回欲しい情報は、「歩行者がカメラに対して同方向を向いているか、そうでないか」だけです。

そこで、

> YOLO-seg自体をファインチューニングし、向きもクラスとして学習させればよいのでは？

という方針にしました。

これが実現できれば、**人検知 + 向き推定 = 1モデル**となり、大幅な軽量化が期待できます。


## YOLO-segとは

### 通常のYOLOとの違い

* 通常のYOLO

  * バウンディングボックス（矩形）で検出
* **YOLO-seg**

  * 物体の**輪郭（ピクセル単位のマスク）**まで出力（インスタンスセグメンテーション）


## COCOデータセットとは

### COCOの特徴

COCO（Common Objects in Context）は、

* 80クラスの物体
* 約33万枚の画像
* 検出・セグメンテーション・キーポイント（人体）付き

という、非常に情報量の多いデータセットです。

### personクラスの特別さ

COCOの `person` クラスには、

* **セグメンテーションマスク**
* **17点のキーポイント**（例：nose / shoulders / elbows / knees / ankles ...）

が付与されています。

この「人のキーポイント情報がある」点が、今回のデータ加工で重要になります。

---

## COCOデータの加工方針（今回の手順）

### ① person画像の抽出（COCO APIを使う）

COCOは `pycocotools`（COCO API）を使うと、

* `category = person`
* さらに `segmentation` や `keypoints` を持つアノテーション

をキーにして、**personを含む画像IDだけ**を抽出できます。

（イメージ）

* `getCatIds(catNms=['person'])`
* `getImgIds(catIds=...)`
* `loadAnns(...)` で `segmentation` / `keypoints` を確認

> 最初から「personを含む画像だけ」に絞れるので、後処理がかなり楽になります。


### ② ノイズ画像のフィルタリング（キーポイント利用）

COCOには、

* 人が小さすぎる
* 手だけ・腕だけ映っている
* 歩行者として不適切

なデータも混ざります。

そこで、**キーポイントを使ったフィルタリング**を行います。

（そのままでもある程度上手く学習できますが、これらのデータは向き推定にはノイズになるため、今回は精度向上のために除外します）

#### フィルタリング条件

以下を満たす歩行者が一人も存在しない場合、学習データから除外

* keypoints の visibility `v ≥ 1`

* 以下のキーポイントが映っている
  * `[5,6,13,14]`
  * [キーポイント一覧表](https://qiita.com/kHz/items/8c06d0cb620f268f4b3e#keypoints-1)
    * 5, 6：左右の**肩**（left/right shoulder）
    * 13, 14：左右の**膝**（left/right knee）

→ **全身がある程度写っている歩行者が含まれる画像を優先**


---

### ③ 任意数のデータを抽出

* 学習用（train）：20000枚
* 検証用（val）：1482枚
* テスト用(test)：3000枚

今回はフィルタリングを通った画像のうち、上記の数のデータを抽出します。

---

### ④ クラス定義（2クラス）

今回のクラスはシンプルです。

```
0: 同方向歩行者
1: 逆方向歩行者
```


## MEBOWとは

### MEBOWの役割

MEBOWは、

* 人のキーポイント情報などを手がかりに
* **身体の向き**を推定するモデル

です。

* 単眼RGB画像
* COCO系データを拡張した大規模データで学習

### 今回の立ち位置

* **学習データ作成用**（教師ラベル生成）
* 推論時には使わない

つまり、

> MEBOWは「教師ラベル生成器」

としてのみ使用します。


## 向きラベルの付与

1. MEBOWで各人物の向きを推定
2. 向き角度をルールで2値化

   * 同方向
     * 角度が 45°以下、または315°以上
   * 逆方向
     * 上記以外
  
3. YOLO-seg用アノテーションに反映

これにより、

* **人マスク**
* **向きクラス**

を同時に持つデータセットが完成します。


## 学習結果

(WIP)


## まとめ

* YOLO-Segをファインチューニングして、用途に合わせた再設計を行いました。
* COCOデータのフィルタリングなども含め、想定以上に簡単にファインチューニングできました。
* 分類したい概念が複雑でなければ、別モデルを併用するのではなくファインチューニングした方が良いかもしれません。


## 参考リンク

* [Ultralytics YOLO Docs](https://docs.ultralytics.com/)
* [COCO Dataset](https://cocodataset.org/)
* [pycocotools（COCO API）](https://github.com/cocodataset/cocoapi)
* [MEBOW](https://arxiv.org/abs/2203.08651)
